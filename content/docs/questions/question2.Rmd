# What student information helps predict success in their courses?

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment='', warning = FALSE)
```

## Set up and load data

```{r}
rm(list =ls())
library(pacman)
p_load(tidyverse, broom, ggplot2, kableExtra, mice, interactions)
load("D:/R/data analysis/Institutional research/technical-exercise/content/docs/questions/data_230214_1659.Rdata")
```

## Research question
**What student information helps predict success in their courses?**

Success in course can be found in the **course_enrollments** data set and be defined as C or better score in their official grade.

The student information can be obtained from **student_details** data set, which include information like **student_id_number**, **ACT_score**, **hs_gpa_entry**, and **hardship_score**. 

## Data cleaning
1. combine **student_details** and **course_enrollments** data set via student_id_number variable.
2. deal with missing values

```{r warning=FALSE}
DF <- student_details %>%
  full_join(course_enrollments, by="student_id_number") %>%
  select(c("student_id_number","ACT_score","hs_gpa_entry","hardship_score","success"))
```

```{r echo=FALSE}
summary(DF)
```

After combining two data sets, the new data set: **DF** has several variables (**ACT_score**, **hs_gpa_entry**, and **success**) with missing values. To have a better model, I first impute those variables.

```{r include=FALSE}
temp <- mice(DF, meth = "pmm", m = 5, maxit = 12, seed = 1804)
summary(temp)
DF <- complete(temp)
```

## Visualization

-   ACT_score

```{r}
DF %>%
  ggplot(aes(x = success, y = ACT_score)) +
  geom_boxplot()+
  labs(
    x = "Success",
    y = "ACT score",
    title = ""
  ) +
  theme_classic()
```

-   hs_gpa_entry

```{r}
DF %>%
  ggplot(aes(x = success, y = hs_gpa_entry)) +
  geom_boxplot()+
  labs(
    x = "Success",
    y = "HS GPA entry",
    title = ""
  ) +
  theme_classic()

```

-   hardship_score

```{r}
DF %>%
  ggplot(aes(x = success, y = hardship_score)) +
  geom_boxplot()+
  labs(
    x = "Success",
    y = "Hardship score",
    title = ""
  ) +
  theme_classic()
```

Three boxplots do not show huge differences on ACT_score, hs_gpa_entry, and hardship_score between students succeed and others.

## Logistic regression
1. Run basic logistic regression model
2. Evaluate interactions
3. Run model with interactions
4. Compare two models

First, I run the basic logistic regression model with three independent variables (**ACT_score**, **hs_gpa_entry**,**hardship_score**) without considering interaction.

### Basic Model

```{r}
DF$success <- relevel((DF$success), ref = "N")
```

```{r}
formula <- as.formula(success~ ACT_score+ hs_gpa_entry+ hardship_score)
lr <- glm(formula, data = DF, family = binomial(link = "logit"))
```

Next, I explore coefficients by calculate odds ratio.

-   Odds ratio is an measure of association between dependent variable and independent variables in the logistic regression. It estimates the change in the odd of membership in the target group for one unit increase in the predictor.

```{r warning=FALSE}
broom::tidy(lr, exp = TRUE) %>%
  kable() %>% 
  kable_styling("basic", bootstrap_options = "striped", full_width = F, position = "left")
```

The results show that all three independent variables are statistically significant associated with the dependent variable (**success**).

-   ACT_score: After controlling all other variables, the odds of success **increase** as ACT score increases.

-   hs_gpa_entry: After controlling all other variables, the odds of success **increase** as hs_gpa_entry increases.

-   hardship_score: After controlling all other variables, the odds of success **decrease** as hardship score increases.

### Add Interactions

Intuitively, those three variables might interact with each other when predicting **success** variable. To verify it, I first examine the interaction. Next, based on the results, I choose the interactions to add in the model.

<details><summary>Evaluate interactions</summary>

-   ACT_score*hardship_score

```{r warning= FALSE}
formulaAH <- as.formula(success~ ACT_score+ hs_gpa_entry+ hardship_score + ACT_score*hardship_score)
lrAH <- glm(formulaAH, data = DF, family = binomial(link = "logit"))
sim_slopes(lrAH, pred = ACT_score, modx = hardship_score, johnson_neyman = TRUE, jnplot = TRUE)
```

From the plot, we can see that for ACT_score, the slope of hardship_score is significantly different from zero and in this case negative.

-   hs_gpa_entry*hardship_score

```{r warning=FALSE}
formulaHH <- as.formula(success~ ACT_score+ hs_gpa_entry+ hardship_score + hs_gpa_entry*hardship_score)
lrHH <- glm(formulaHH, data = DF, family = binomial(link = "logit"))
sim_slopes(lrHH, pred = hs_gpa_entry, modx = hardship_score, johnson_neyman = TRUE, jnplot = TRUE)
```

For hs_gpa_entry, the slope of hardship_score is also significantly different from zero and positive. Overall, those two interactions are significant. So, I model these interactions and compare the new model with the previous basic model.
</details>


<details><summary>Run model with interactions</summary>

```{r}
formulaFull<- as.formula(success~ ACT_score+ hs_gpa_entry+ hardship_score + ACT_score*hardship_score + hs_gpa_entry*hardship_score)
lrFull <- glm(formulaFull, data = DF, family = binomial(link = "logit"))
```

Odds ratio estimates the change in the odd of membership in the target group for one unit increase in the predictor.

```{r warning=FALSE}
broom::tidy(lrFull, exp = TRUE) %>%
  kable() %>% 
  kable_styling("basic", bootstrap_options = "striped", full_width = F, position = "left")
```
</details>

All variables, including interactions are statistically significant associated with the dependent variable


### Compare two models

To choose from those two models, I compare several criteria of them. One is AIC, which is a commonly used measurement to compare models; lower values are considered better fitting than those with larger values.

Another is pseudo R-square, which provides model fit information. 

-   AIC

```{r}
glance(lr)
glance(lrFull)
```

Those two tables show that the model with interactions has smaller AIC (138714.9) than the basic model (AIC = 138761.6).

-   Pseudo R-Square

```{r}
DescTools::PseudoR2(lr, which =  "all")
DescTools::PseudoR2(lrFull, which =  "all")
```

Results of pseudo R-square tables show that the basic model accounts for approximately 32.9% of the total variance; the model with interactions accounts for approximately 33.53% of the total variance.

## Conclusion

In summary, the model with interactions has better performance. Variables (**ACT_score**, **hs_gpa_entry**, **hardship_score**) and interactions ( **ACT_score x hardship_score**, **hs_gpa_entry x hardship_score**) significantly predict success.
